:imagesdir: img/post_deployment

# Post deployment steps: After stack creation is completed, what does user needs to do to see the value of this QS? Instructions with screenshots as needed.

To see the capabilities of the QuickStart it can be tested with a dataset of meter reads from the City of London between 2011 to 2014.

## Download Sample Dataset
The sample dataset can be downloaded from https://data.london.gov.uk/dataset/smartmeter-energy-use-data-in-london-households[here]. The file can be downloaded to your local machine unzipped and then uploaded to S3. However, given the size of the unzipped file (~11GB), it is faster to run a EC2 instance with sufficient privileges to write to S3, download the file to the EC2 instance, unzip it and upload the data from there.

https://data.london.gov.uk/download/smartmeter-energy-use-data-in-london-households/3527bf39-d93e-4071-8451-df2ade1ea4f2/Power-Networks-LCL-June2015(withAcornGps).zip[Download]

## Upload the dataset to the landing zone
The sample dataset needs to be uploaded to the landing zone bucket. The landing zone is the starting point for all meter reads. Files that are placed in this folder will be picked up automatically by the pipeline and processed. For that it doesn't matter if one large file or multiple small once are used, the process takes care of it. Furthermore, the AWS Glue ETL workflow tracks which files haven been processed and which not, there is no need to handle this separately.

1. Select the landing zone bucket, buckets names are generated, find the one which contains the word landing zone. This will be the starting point for the ETL process.
image:1_bucket_layout.png[Bucket Layout]

2. Upload the London Meter Data to the landin zone bucket.
image:2_upload_demo_data_set.png[Upload demo data set]

3. After the demo data is uploaded, the folder contains one file with all the meter reads.
image:3_upload_demo_data_set.png[Uploaded demo data set]

[TODO EC2 command line steps]

## Start the AWS Glue ETL and ML training pipeline

In the default configuration the pipeline will be triggered each day at 09:00AM to work on data that has been new arrived to the landing zone. It's also possible to trigger the pipeline manually. For that we first switch to the AWS Glue console and the select the 'Workflow' menu entry. The workflow defines the orchestrates the different steps of the ETL process, when the workflow has successfully been finished a state machine https://state.machine[link] is called to build the ML model and prepare the forecast and outages data for prediction.

1. To start the AWS Glue workflow manually, navigate to the AWS Glue Console, and select 'Workflows'
image:4_start_etl_workflow.png[AWS Glue Console]

2. Select the actual workflow and choose 'Run' from the drop-down-menu
image:5_start_etl_workflow.png[Start ETL workflow]

3. After started, the Workflow should be indicated as 'Running' in the 'History' tab
image:6_start_etl_workflow.png[Start ETL workflow]

TIP: If the workflow doesn't start and jumps directly to 'Completed' go back to step 2